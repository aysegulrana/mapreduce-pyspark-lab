{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import time\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_spark_context():\n",
    "    \"\"\"Initialize and return a SparkContext for the application\"\"\"\n",
    "    # Configure Spark\n",
    "    conf = SparkConf().setAppName(\"MapReduceLab\").setMaster(\"local[*]\")\n",
    "    # Create Spark context\n",
    "    sc = SparkContext(conf=conf)\n",
    "    # Set log level to reduce console output\n",
    "    sc.setLogLevel(\"ERROR\")\n",
    "    return sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_mapreduce(sc, input_file):\n",
    "    \"\"\"Perform word count using PySpark's MapReduce approach\"\"\"\n",
    "    # Time the operation\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 1. Read the input file into an RDD\n",
    "    lines_rdd = sc.textFile(input_file)\n",
    "\n",
    "    print(\"Number of partitions: \", lines_rdd.getNumPartitions())\n",
    "    print(\"Total number of lines: \", lines_rdd.count())\n",
    "\n",
    "    print(\"First 10 lines of the input file:\")\n",
    "    for i, line in enumerate(lines_rdd.take(10)):\n",
    "        print(f\"Line {i}: {line}\")\n",
    "    \n",
    "    # 2. MAP phase: Split each line into words and create (word, 1) pairs\n",
    "    words_rdd = lines_rdd.flatMap(lambda line: line.split())\n",
    "    word_pairs_rdd = words_rdd.map(lambda word: (word, 1))\n",
    "\n",
    "    print(\"First 10 word pairs:\")\n",
    "    for i, word_pair in enumerate(word_pairs_rdd.take(10)):\n",
    "        print(f\"Word pair {i}: {word_pair}\")\n",
    "    \n",
    "    # 3. REDUCE phase: Sum the counts for each word\n",
    "    word_counts_rdd = word_pairs_rdd.reduceByKey(lambda a, b: a + b) #the key is the word, the value is the count\n",
    "    \n",
    "    # 4. Sort results by count (descending)\n",
    "    sorted_counts = word_counts_rdd.sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "    print(\"First 10 word counts:\")\n",
    "    for i, word_count in enumerate(sorted_counts.take(10)):\n",
    "        print(f\"Word count {i}: {word_count}\")\n",
    "    \n",
    "    # 5. Collect results to driver program (this triggers execution)\n",
    "    result = sorted_counts.collect()\n",
    "    \n",
    "    # Calculate execution time\n",
    "    execution_time = time.time() - start_time\n",
    "    \n",
    "    return result, execution_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_single_machine(input_file):\n",
    "    \"\"\"Perform word count using a traditional single-machine approach\"\"\"\n",
    "    # Time the operation\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Dictionary to store word counts\n",
    "    word_counts = {}\n",
    "    \n",
    "    # Read the file and count words\n",
    "    with open(input_file, 'r') as f:\n",
    "        for line in f:\n",
    "            for word in line.strip().split():\n",
    "                if word in word_counts:\n",
    "                    word_counts[word] += 1\n",
    "                else:\n",
    "                    word_counts[word] = 1\n",
    "    \n",
    "    # Sort results by count\n",
    "    sorted_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Calculate execution time\n",
    "    execution_time = time.time() - start_time\n",
    "    \n",
    "    return sorted_counts, execution_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_partitioning(sc, input_file, num_partitions=4):\n",
    "    \"\"\"Demonstrate how data is partitioned in Spark\"\"\"\n",
    "    # Create an RDD with specified number of partitions\n",
    "    lines_rdd = sc.textFile(input_file, minPartitions=num_partitions)\n",
    "    \n",
    "    # Show partition information\n",
    "    print(f\"Number of partitions: {lines_rdd.getNumPartitions()}\")\n",
    "    \n",
    "    # Map each item with its partition ID to visualize distribution\n",
    "    def get_partition_info(item, partition_id):\n",
    "        return (partition_id, item)\n",
    "    \n",
    "    # Use mapPartitionsWithIndex to get partition information\n",
    "    partition_info = lines_rdd.mapPartitionsWithIndex(\n",
    "        lambda idx, items: [(idx, sum(1 for _ in items))],\n",
    "        preservesPartitioning=True\n",
    "    ).collect()\n",
    "    \n",
    "    print(\"Items per partition:\")\n",
    "    for partition_id, count in partition_info:\n",
    "        print(f\"  Partition {partition_id}: {count} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "MAPREDUCE LAB - DISTRIBUTED SYSTEMS\n",
      "==================================================\n",
      "\n",
      "Part 1: Word Count with PySpark MapReduce\n",
      "----------------------------------------\n",
      "Number of partitions:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of lines:  4080\n",
      "First 10 lines of the input file:\n",
      "Line 0: \n",
      "Line 1: \n",
      "Line 2: STAR WARS EPISODE 3: REVENGE OF THE SITH SCRIPT \n",
      "Line 3: \n",
      "Line 4: George Lucas \n",
      "Line 5: \n",
      "Line 6: \n",
      "Line 7: 1 EXT. SPACE \n",
      "Line 8: \n",
      "Line 9: A long time ago in a galaxy far, far away. \n",
      "First 10 word pairs:\n",
      "Word pair 0: ('STAR', 1)\n",
      "Word pair 1: ('WARS', 1)\n",
      "Word pair 2: ('EPISODE', 1)\n",
      "Word pair 3: ('3:', 1)\n",
      "Word pair 4: ('REVENGE', 1)\n",
      "Word pair 5: ('OF', 1)\n",
      "Word pair 6: ('THE', 1)\n",
      "Word pair 7: ('SITH', 1)\n",
      "Word pair 8: ('SCRIPT', 1)\n",
      "Word pair 9: ('George', 1)\n",
      "First 10 word counts:\n",
      "Word count 0: ('the', 1723)\n",
      "Word count 1: ('.', 805)\n",
      "Word count 2: ('to', 704)\n",
      "Word count 3: ('and', 675)\n",
      "Word count 4: ('of', 522)\n",
      "Word count 5: ('a', 467)\n",
      "Word count 6: ('I', 387)\n",
      "Word count 7: ('is', 351)\n",
      "Word count 8: ('The', 315)\n",
      "Word count 9: ('ANAKIN:', 312)\n",
      "\n",
      "Top 10 words by frequency:\n",
      "  the: 1723\n",
      "  .: 805\n",
      "  to: 704\n",
      "  and: 675\n",
      "  of: 522\n",
      "  a: 467\n",
      "  I: 387\n",
      "  is: 351\n",
      "  The: 315\n",
      "  ANAKIN:: 312\n",
      "\n",
      "PySpark execution time: 1.01 seconds\n",
      "\n",
      "Part 2: Comparison with Single-Machine Processing\n",
      "----------------------------------------\n",
      "Single-machine execution time: 0.01 seconds\n",
      "Speed improvement with PySpark: 0.01x\n",
      "\n",
      "Part 3: Understanding Data Partitioning\n",
      "----------------------------------------\n",
      "Number of partitions: 4\n",
      "Items per partition:\n",
      "  Partition 0: 992 items\n",
      "  Partition 1: 1124 items\n",
      "  Partition 2: 901 items\n",
      "  Partition 3: 1063 items\n",
      "\n",
      "Part 4: Effect of Increasing Partitions\n",
      "----------------------------------------\n",
      "Execution with 2 partitions: 0.08 seconds\n",
      "Execution with 4 partitions: 0.25 seconds\n",
      "Execution with 8 partitions: 0.69 seconds\n",
      "Execution with 16 partitions: 0.44 seconds\n"
     ]
    }
   ],
   "source": [
    "def main_processor(word_count_mr, word_count_single):\n",
    "    \"\"\"Main function to run the MapReduce lab\"\"\"\n",
    "    data_dir = \"data\"\n",
    "    input_file = f\"{data_dir}/RevengeoftheSithScript.txt\"\n",
    "    \n",
    "    # Initialize Spark context\n",
    "    sc = setup_spark_context()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"MAPREDUCE LAB - DISTRIBUTED SYSTEMS\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    # Part 1: Demonstrate basic word count using PySpark\n",
    "    print(\"Part 1: Word Count with PySpark MapReduce\")\n",
    "    print(\"-\"*40)\n",
    "    spark_result, spark_time = word_count_mr(sc, input_file)\n",
    "    \n",
    "    # Display top 10 words\n",
    "    print(\"\\nTop 10 words by frequency:\")\n",
    "    for word, count in spark_result[:10]:\n",
    "        print(f\"  {word}: {count}\")\n",
    "    \n",
    "    print(f\"\\nPySpark execution time: {spark_time:.2f} seconds\")\n",
    "    \n",
    "    # Part 2: Compare with single-machine approach\n",
    "    print(\"\\nPart 2: Comparison with Single-Machine Processing\")\n",
    "    print(\"-\"*40)\n",
    "    single_result, single_time = word_count_single(input_file)\n",
    "    \n",
    "    print(f\"Single-machine execution time: {single_time:.2f} seconds\")\n",
    "    print(f\"Speed improvement with PySpark: {single_time/spark_time:.2f}x\")\n",
    "    \n",
    "    # Part 3: Demonstrate partitioning\n",
    "    print(\"\\nPart 3: Understanding Data Partitioning\")\n",
    "    print(\"-\"*40)\n",
    "    demonstrate_partitioning(sc, input_file)\n",
    "    \n",
    "    # Part 4: Demonstrate the effect of increasing partitions\n",
    "    print(\"\\nPart 4: Effect of Increasing Partitions\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # Try different numbers of partitions\n",
    "    partition_sizes = [2, 4, 8, 16]\n",
    "    times = []\n",
    "    \n",
    "    for partitions in partition_sizes:\n",
    "        # Configure RDD with specific number of partitions\n",
    "        lines_rdd = sc.textFile(input_file, minPartitions=partitions)\n",
    "        words_rdd = lines_rdd.flatMap(lambda line: line.split())\n",
    "        word_pairs_rdd = words_rdd.map(lambda word: (word, 1))\n",
    "        \n",
    "        # Time the reduceByKey operation with different partition counts\n",
    "        start_time = time.time()\n",
    "        word_counts_rdd = word_pairs_rdd.reduceByKey(lambda a, b: a + b, numPartitions=partitions)\n",
    "        result = word_counts_rdd.collect()\n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        times.append(execution_time)\n",
    "        print(f\"Execution with {partitions} partitions: {execution_time:.2f} seconds\")\n",
    "    \n",
    "    # Clean up\n",
    "    sc.stop()\n",
    "    \n",
    "main_processor(word_count_mr=word_count_mapreduce, word_count_single=word_count_single_machine)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set([\n",
    "\"the\", \"a\", \"an\", \"and\", \"or\", \"but\", \"in\", \"on\", \"at\", \"to\", \"for\", \"with\", \n",
    "\"by\", \"of\", \"is\", \"was\", \"were\", \"be\", \"been\", \"being\", \"am\", \"are\", \"this\", \n",
    "\"that\", \"these\", \"those\", \"it\", \"its\", \"from\", \"as\", \"i\", \"he\", \"she\", \"they\", \n",
    "\"we\", \"you\", \"him\", \"her\", \"them\", \"their\", \"our\", \"your\", \"my\", \"has\", \"have\"\n",
    "])\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convert to lowercase and replace punctuation with spaces\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)  # Replace punctuation with space\n",
    "    # Split by whitespace and filter out empty strings\n",
    "    words = [word for word in text.split() if word]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_single_machine_with_cleaning(input_file):\n",
    "    \"\"\"Perform word count with cleaning using a traditional single-machine approach\"\"\"\n",
    "    # Time the operation\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Import regular expressions for punctuation removal\n",
    "    \n",
    "    \n",
    "    # Dictionary to store word counts\n",
    "    word_counts = {}\n",
    "    \n",
    "    # Read the file and count words\n",
    "    with open(input_file, 'r') as f:\n",
    "        for line in f:\n",
    "            # Clean text (convert to lowercase and remove punctuation)\n",
    "            line = clean_text(line)\n",
    "            \n",
    "            # Split into words\n",
    "            for word in line.split():\n",
    "                # Apply filtering (stop words and length)\n",
    "                if word not in stop_words and len(word) > 1:\n",
    "                    if word in word_counts:\n",
    "                        word_counts[word] += 1\n",
    "                    else:\n",
    "                        word_counts[word] = 1\n",
    "    \n",
    "    # Sort results by count\n",
    "    sorted_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Calculate execution time\n",
    "    execution_time = time.time() - start_time\n",
    "    \n",
    "    return sorted_counts, execution_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_with_cleaning(sc, input_file):\n",
    "    \"\"\"Perform word count with stop words and punctuation removal using PySpark's MapReduce approach\"\"\"\n",
    "    # Time the operation\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Define common stop words\n",
    "    stop_words = set([\n",
    "        \"the\", \"a\", \"an\", \"and\", \"or\", \"but\", \"in\", \"on\", \"at\", \"to\", \"for\", \"with\", \n",
    "        \"by\", \"of\", \"is\", \"was\", \"were\", \"be\", \"been\", \"being\", \"am\", \"are\", \"this\", \n",
    "        \"that\", \"these\", \"those\", \"it\", \"its\", \"from\", \"as\", \"i\", \"he\", \"she\", \"they\", \n",
    "        \"we\", \"you\", \"him\", \"her\", \"them\", \"their\", \"our\", \"your\", \"my\", \"has\", \"have\"\n",
    "    ])\n",
    "\n",
    "    \n",
    "    # 1. Read the input file into an RDD\n",
    "    lines_rdd = sc.textFile(input_file)\n",
    "    \n",
    "    # 2. MAP phase: Clean text, split into words, remove stop words, and create (word, 1) pairs\n",
    "    words_rdd = lines_rdd.flatMap(clean_text)\n",
    "\n",
    "    print(\"First 10 words:\")\n",
    "    for i, word in enumerate(words_rdd.take(10)):\n",
    "        print(f\"Word {i}: {word}\")\n",
    "\n",
    "    filtered_words_rdd = words_rdd.filter(lambda word: word not in stop_words and len(word) > 1)\n",
    "\n",
    "\n",
    "    word_pairs_rdd = filtered_words_rdd.map(lambda word: (word, 1))\n",
    "\n",
    "    print(\"First 10 word pairs:\")\n",
    "    for i, word_pair in enumerate(word_pairs_rdd.take(10)):\n",
    "        print(f\"Word pair {i}: {word_pair}\")\n",
    "    \n",
    "    # 3. REDUCE phase: Sum the counts for each word\n",
    "    word_counts_rdd = word_pairs_rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "\n",
    "    \n",
    "    # 4. Sort results by count (descending)\n",
    "    sorted_counts = word_counts_rdd.sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "    print(\"First 10 word counts:\")\n",
    "    for i, word_count in enumerate(sorted_counts.take(10)):\n",
    "        print(f\"Word count {i}: {word_count}\")\n",
    "    \n",
    "    # 5. Collect results to driver program (this triggers execution)\n",
    "    result = sorted_counts.collect()\n",
    "    \n",
    "    # Calculate execution time\n",
    "    execution_time = time.time() - start_time\n",
    "    \n",
    "    return result, execution_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "MAPREDUCE LAB - DISTRIBUTED SYSTEMS\n",
      "==================================================\n",
      "\n",
      "Part 1: Word Count with PySpark MapReduce\n",
      "----------------------------------------\n",
      "First 10 words:\n",
      "Word 0: star\n",
      "Word 1: wars\n",
      "Word 2: episode\n",
      "Word 3: 3\n",
      "Word 4: revenge\n",
      "Word 5: of\n",
      "Word 6: the\n",
      "Word 7: sith\n",
      "Word 8: script\n",
      "Word 9: george\n",
      "First 10 word pairs:\n",
      "Word pair 0: ('star', 1)\n",
      "Word pair 1: ('wars', 1)\n",
      "Word pair 2: ('episode', 1)\n",
      "Word pair 3: ('revenge', 1)\n",
      "Word pair 4: ('sith', 1)\n",
      "Word pair 5: ('script', 1)\n",
      "Word pair 6: ('george', 1)\n",
      "Word pair 7: ('lucas', 1)\n",
      "Word pair 8: ('ext', 1)\n",
      "Word pair 9: ('space', 1)\n",
      "First 10 word counts:\n",
      "Word count 0: ('anakin', 701)\n",
      "Word count 1: ('wan', 566)\n",
      "Word count 2: ('obi', 566)\n",
      "Word count 3: ('jedi', 256)\n",
      "Word count 4: ('padme', 251)\n",
      "Word count 5: ('his', 246)\n",
      "Word count 6: ('palpatine', 208)\n",
      "Word count 7: ('int', 153)\n",
      "Word count 8: ('yoda', 140)\n",
      "Word count 9: ('will', 134)\n",
      "\n",
      "Top 10 words by frequency:\n",
      "  anakin: 701\n",
      "  wan: 566\n",
      "  obi: 566\n",
      "  jedi: 256\n",
      "  padme: 251\n",
      "  his: 246\n",
      "  palpatine: 208\n",
      "  int: 153\n",
      "  yoda: 140\n",
      "  will: 134\n",
      "\n",
      "PySpark execution time: 0.87 seconds\n",
      "\n",
      "Part 2: Comparison with Single-Machine Processing\n",
      "----------------------------------------\n",
      "Single-machine execution time: 0.02 seconds\n",
      "Speed improvement with PySpark: 0.02x\n",
      "\n",
      "Part 3: Understanding Data Partitioning\n",
      "----------------------------------------\n",
      "Number of partitions: 4\n",
      "Items per partition:\n",
      "  Partition 0: 992 items\n",
      "  Partition 1: 1124 items\n",
      "  Partition 2: 901 items\n",
      "  Partition 3: 1063 items\n",
      "\n",
      "Part 4: Effect of Increasing Partitions\n",
      "----------------------------------------\n",
      "Execution with 2 partitions: 0.05 seconds\n",
      "Execution with 4 partitions: 0.13 seconds\n",
      "Execution with 8 partitions: 0.13 seconds\n",
      "Execution with 16 partitions: 0.47 seconds\n"
     ]
    }
   ],
   "source": [
    "main_processor(word_count_mr=word_count_with_cleaning, word_count_single=word_count_single_machine_with_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADJUST THE MB SIZE TO GENERATE A LARGE OR SMALLER FILE\n",
    "def generate_large_sample_data(filename, size_in_mb=500):\n",
    "    \"\"\"Generate a large sample text file with random words.\"\"\"\n",
    "    print(f\"Generating large sample data (~{size_in_mb}MB)...\")\n",
    "    \n",
    "    # List of sample words to generate random text\n",
    "    words = [\"hadoop\", \"spark\", \"distributed\", \"computing\", \"mapreduce\", \n",
    "             \"cluster\", \"data\", \"algorithm\", \"parallel\", \"processing\",\n",
    "             \"scale\", \"node\", \"system\", \"framework\", \"cloud\", \"partition\",\n",
    "             \"memory\", \"storage\", \"driver\", \"executor\", \"job\", \"task\",\n",
    "             \"shuffle\", \"reduce\", \"map\", \"cache\", \"persist\", \"lineage\",\n",
    "             \"transformation\", \"action\", \"rdd\", \"dataframe\", \"dataset\"]\n",
    "    \n",
    "    # Calculate approximate number of lines needed\n",
    "    # Assuming average 6 words per line and average 6 chars per word + spaces\n",
    "    chars_per_line = 6 * 6 + 5  # words * avg_word_length + spaces\n",
    "    lines_needed = (size_in_mb * 1024 * 1024) // chars_per_line\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    \n",
    "    # Generate the file with random words\n",
    "    with open(filename, 'w') as f:\n",
    "        for i in range(lines_needed):\n",
    "            # Generate a random line with 4-8 words\n",
    "            line_length = random.randint(4, 8)\n",
    "            line = ' '.join(random.choice(words) for _ in range(line_length))\n",
    "            f.write(line + '\\n')\n",
    "            \n",
    "            # Print progress every million lines\n",
    "            if i % 1000000 == 0 and i > 0:\n",
    "                print(f\"  Generated {i:,} lines (~{i*chars_per_line/(1024*1024):.1f}MB)\")\n",
    "    \n",
    "    file_size_mb = os.path.getsize(filename) / (1024 * 1024)\n",
    "    print(f\"Large sample data generated and saved to {filename} ({file_size_mb:.1f}MB)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
